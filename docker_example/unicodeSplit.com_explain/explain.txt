结巴中文分词（PHP 版本)：做最好的 PHP 中文分词、中文断词语组件(composer )
2017-02-25 07:20:54    Laravel

  中文分词   composer   结巴分词   PHP 中文分詞   中文断词语組件

结巴中文分词（PHP 版本）：做最好的 PHP 中文分詞、中文断词组件
结巴分词组件：fukuball/jieba-php

网址：https://packagist.org/packages/fukuball/jieba-php

利用composer 拉去结巴分词组件

composer require fukuball/jieba-php:dev-master
拉取成功以后 会在vendor文件夹里 发现fukuball目录，这个就是结巴分词组件，

然后在控制器里引入

use Fukuball\Jieba\Jieba;
use Fukuball\Jieba\Finalseg;
public function jieba(Request $request)
{
    $paragraph = $request->input('paragraph');
    Jieba::init(array(
        'mode'=>'default',
        'dict'=>'samll'
    ));
    Finalseg::init();//
    $seg_list = Jieba::cut($paragraph);
    dd($seg_list);
}
原始文章

    中文分词(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂的多、困难的多。
分词后的

    array:102 [▼
      0 => "中文"
      1 => "分词"
      2 => "Chinese"
      3 => "Word"
      4 => "Segmentation"
      5 => "指"
      6 => "的"
      7 => "是"
      8 => "将"
      9 => "一个"
      10 => "汉字"
      11 => "序列"
      12 => "切"
      13 => "分成"
      14 => "一个"
      15 => "一个"
      16 => "单独"
      17 => "的"
      18 => "词"
      19 => "分词"
      20 => "就是"
      21 => "将"
      22 => "连续"
      23 => "的"
      24 => "字"
      25 => "序列"
      26 => "按照"
      27 => "一定"
      28 => "的"
      29 => "规范"
      30 => "重新组合"
      31 => "成词"
      32 => "序列"
      33 => "的"
      34 => "过程"
      35 => "我们"
      36 => "知道"
      37 => "在"
      38 => "英文"
      39 => "的"
      40 => "行文"
      41 => "中"
      42 => "单词"
      43 => "之间"
      44 => "是"
      45 => "以"
      46 => "空格"
      47 => "作为"
      48 => "自然"
      49 => "分界"
      50 => "符"
      51 => "的"
      52 => "而"
      53 => "中文"
      54 => "只是"
      55 => "字"
      56 => "句"
      57 => "和"
      58 => "段"
      59 => "能"
      60 => "通过"
      61 => "明显"
      62 => "的"
      63 => "分界"
      64 => "符来"
      65 => "简单"
      66 => "划界"
      67 => "唯独"
      68 => "词"
      69 => "没有"
      70 => "一个"
      71 => "形式"
      72 => "上"
      73 => "的"
      74 => "分界"
      75 => "符"
      76 => "虽然"
      77 => "英文"
      78 => "也"
      79 => "同样"
      80 => "存在"
      81 => "短语"
      82 => "的"
      83 => "划分"
      84 => "问题"
      85 => "不过"
      86 => "在"
      87 => "词"
      88 => "这"
      89 => "一层"
      90 => "上"
      91 => "中文"
      92 => "比"
      93 => "之"
      94 => "英文"
      95 => "要"
      96 => "复杂"
      97 => "的"
      98 => "多"
      99 => "困难"
      100 => "的"
      101 => "多"
    ]
功能1 : 分词
  cut 方法接受多个输入参数：
      1) 第一个参数为需要分词的字符串
      2）cut_all 参数用来控制分词模式
  cutForSearch 方法接受一个参数：需要分词的字符串，该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细
  注意：待分词的字符串是 utf-8 字符串
  cut 以及 cutForSearch 返回的结构是一个可迭代的 array
功能2 : 添加自定义词典

  开发者可以指定自己自定义的词典，以便包含 jieba 词库裡没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率
  用法： Jieba::loadUserDict(file_name) # file_name 为自定义词典的绝对路径
  词典格式和 dict.txt 一样，一个词佔一行；每一行分为三部分，一部分为词语，一部分为词频，一部分为词性，用空格隔开
  事例： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 / 加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /
原始文章

    中文分词(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂的多、困难的多。
分词后的

    array:102 [▼
      0 => "中文"
      1 => "分词"
      2 => "Chinese"
      3 => "Word"
      4 => "Segmentation"
      5 => "指"
      6 => "的"
      7 => "是"
      8 => "将"
      9 => "一个"
      10 => "汉字"
      11 => "序列"
      12 => "切"
      13 => "分成"
      14 => "一个"
      15 => "一个"
      16 => "单独"
      17 => "的"
      18 => "词"
      19 => "分词"
      20 => "就是"
      21 => "将"
      22 => "连续"
      23 => "的"
      24 => "字"
      25 => "序列"
      26 => "按照"
      27 => "一定"
      28 => "的"
      29 => "规范"
      30 => "重新组合"
      31 => "成词"
      32 => "序列"
      33 => "的"
      34 => "过程"
      35 => "我们"
      36 => "知道"
      37 => "在"
      38 => "英文"
      39 => "的"
      40 => "行文"
      41 => "中"
      42 => "单词"
      43 => "之间"
      44 => "是"
      45 => "以"
      46 => "空格"
      47 => "作为"
      48 => "自然"
      49 => "分界"
      50 => "符"
      51 => "的"
      52 => "而"
      53 => "中文"
      54 => "只是"
      55 => "字"
      56 => "句"
      57 => "和"
      58 => "段"
      59 => "能"
      60 => "通过"
      61 => "明显"
      62 => "的"
      63 => "分界"
      64 => "符来"
      65 => "简单"
      66 => "划界"
      67 => "唯独"
      68 => "词"
      69 => "没有"
      70 => "一个"
      71 => "形式"
      72 => "上"
      73 => "的"
      74 => "分界"
      75 => "符"
      76 => "虽然"
      77 => "英文"
      78 => "也"
      79 => "同样"
      80 => "存在"
      81 => "短语"
      82 => "的"
      83 => "划分"
      84 => "问题"
      85 => "不过"
      86 => "在"
      87 => "词"
      88 => "这"
      89 => "一层"
      90 => "上"
      91 => "中文"
      92 => "比"
      93 => "之"
      94 => "英文"
      95 => "要"
      96 => "复杂"
      97 => "的"
      98 => "多"
      99 => "困难"
      100 => "的"
      101 => "多"
    ]
功能1 : 分词
  cut 方法接受多个输入参数：
      1) 第一个参数为需要分词的字符串
      2）cut_all 参数用来控制分词模式
  cutForSearch 方法接受一个参数：需要分词的字符串，该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细
  注意：待分词的字符串是 utf-8 字符串
  cut 以及 cutForSearch 返回的结构是一个可迭代的 array
功能2 : 添加自定义词典

  开发者可以指定自己自定义的词典，以便包含 jieba 词库裡没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率
  用法： Jieba::loadUserDict(file_name) # file_name 为自定义词典的绝对路径
  词典格式和 dict.txt 一样，一个词佔一行；每一行分为三部分，一部分为词语，一部分为词频，一部分为词性，用空格隔开
  事例： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 / 加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /